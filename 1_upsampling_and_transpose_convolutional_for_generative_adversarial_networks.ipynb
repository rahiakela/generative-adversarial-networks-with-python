{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-upsampling-and-transpose-convolutional-for-generative-adversarial-networks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNSyPQxQf0FegyiYLzFuLdY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/generative-adversarial-networks-with-python/blob/part-1-foundations/1_upsampling_and_transpose_convolutional_for_generative_adversarial_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO2OoKmSR0WT",
        "colab_type": "text"
      },
      "source": [
        "# Upsampling and Transpose Convolutional  for Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI5OT5XTR5sR",
        "colab_type": "text"
      },
      "source": [
        "The GAN architecture is comprised of both a generator and a discriminator model. The generator is responsible for creating new outputs, such as images, that plausibly could have come from the original dataset. The generator model is typically implemented using a deep convolutional neural network and\n",
        "results-specialized layers that learn to fill in features in an image rather than extract features from an input image.\n",
        "\n",
        "Two common types of layers that can be used in the generator model are a upsample layer that simply doubles the dimensions of the input and the transpose convolutional layer that performs an inverse convolution operation.\n",
        "\n",
        "In this notebook, we will discover how to use Upsampling and Transpose Convolutional Layers in Generative Adversarial Networks when generating images. \n",
        "\n",
        "After completing this guide, we will know:\n",
        "\n",
        "* Generative models in the GAN architecture are required to upsample input data in order to generate an output image.\n",
        "\n",
        "* The Upsampling layer is a simple layer with no weights that will double the dimensions of input and can be used in a generative model when followed by a traditional convolutional layer.\n",
        "\n",
        "* The Transpose Convolutional layer is an inverse convolutional layer that will both upsample input and learn how to fill in details during the model training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS9gL_XJTbQl",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-FndCvuTcXf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "54e70bd0-f00a-4bf4-8593-d3d7198890fd"
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import UpSampling2D, Dense, Reshape, Conv2D\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n",
            "Go to Runtime > Change runtime and select a GPU hardware accelerator.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHZm8KRUTtMj",
        "colab_type": "text"
      },
      "source": [
        "## Need for Upsampling in GANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I0iJRRNTw79",
        "colab_type": "text"
      },
      "source": [
        "Generative Adversarial Networks are an architecture for neural networks for training a generative model. The architecture is comprised of a generator and a discriminator model, each of which are implemented as a deep convolutional neural network. \n",
        "\n",
        "The discriminator is responsible for classifying images as either real (from the domain) or fake (generated). \n",
        "\n",
        "The generator is responsible for generating new plausible examples from the problem domain. The generator works by taking a random point from the latent space as input and outputting a complete image, in a one-shot manner.\n",
        "\n",
        "A traditional convolutional neural network for image classification, and related tasks, will use pooling layers to downsample input images. For example, an average pooling or max pooling layer will reduce the feature maps from a convolutional by half on each dimension, resulting in an output that is one quarter the area of the input. \n",
        "\n",
        "Convolutional layers themselves also perform a form of downsampling by applying each filter across the input images or feature maps; the resulting activations are an output feature map that is smaller because of the border effects.\n",
        "Often padding is used to counter this effect. \n",
        "\n",
        "The generator model in a GAN requires an inverse operation of a pooling layer in a traditional convolutional layer. It needs a layer to translate\n",
        "from coarse salient features to a more dense and detailed output.\n",
        "\n",
        "A simple version of an unpooling or opposite pooling layer is called an upsampling layer. It works by repeating the rows and columns of the input. A more elaborate approach is to perform a backwards convolutional operation, originally referred to as a deconvolution, which is incorrect, but is more commonly referred to as a fractional convolutional layer or a transposed\n",
        "convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHrMP7j7Ug21",
        "colab_type": "text"
      },
      "source": [
        "## How to Use the Upsampling Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYHTtuCTUh6p",
        "colab_type": "text"
      },
      "source": [
        "Perhaps the simplest way to upsample an input is to double each row and column. \n",
        "\n",
        "For example, an input image with the shape 2 × 2 would be output as 4 × 4.\n",
        "\n",
        "```python\n",
        "input = [\n",
        "  [1, 2],\n",
        "  [3, 4]\n",
        "]\n",
        "\n",
        "Output = [\n",
        "  [1, 1, 2, 2],\n",
        "  [1, 1, 2, 2],\n",
        "  [3, 3, 4, 4],\n",
        "  [3, 3, 4, 4]\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb0OhdQiVTM_",
        "colab_type": "text"
      },
      "source": [
        "### Worked Example Using the UpSampling2D Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwIR4BHHVUOB",
        "colab_type": "text"
      },
      "source": [
        "The Keras deep learning library provides this capability in a layer called UpSampling2D. It can be added to a convolutional neural network and repeats the rows and columns provided as input in the output.\n",
        "\n",
        "**Step-1**\n",
        "\n",
        "First, we can define a contrived input image that is 2 × 2 pixels. We can use specific values for each pixel so that after upsampling, we can see exactly what effect the operation had on the input.\n",
        "\n",
        "**Step-2**\n",
        "\n",
        "Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model. The data dimensions in order are: samples, rows, columns, and channels.\n",
        "\n",
        "**Step-3**\n",
        "\n",
        "We can now define our model. The model has only the UpSampling2D layer which takes 2 × 2 grayscale images as input directly and outputs the result of the upsampling operation.\n",
        "\n",
        "**Step-4**\n",
        "\n",
        "We can then use the model to make a prediction, that is upsample a provided input image.\n",
        "\n",
        "**Step-5**\n",
        "\n",
        "The output will have four dimensions, like the input, therefore, we can convert it back to a 2 × 2 array to make it easier to review the result.\n",
        "\n",
        "Tying all of this together, the complete example of using the UpSampling2D layer in Keras is provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTLyuHPTWL4U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "e6de900c-f961-4284-97cd-ce7eedcc3aad"
      },
      "source": [
        "# step-1: define input data\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [3, 4]          \n",
        "])\n",
        "print(X)\n",
        "\n",
        "# step-2: reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "\n",
        "# step-3: define model\n",
        "model = Sequential()\n",
        "model.add(UpSampling2D(input_shape=(2, 2, 1)))\n",
        "model.summary()\n",
        "\n",
        "# step-4: make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# step-5: reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 4))\n",
        "print(yhat)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "up_sampling2d (UpSampling2D) (None, 4, 4, 1)           0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[1. 1. 2. 2.]\n",
            " [1. 1. 2. 2.]\n",
            " [3. 3. 4. 4.]\n",
            " [3. 3. 4. 4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAWYZxw1Xn1_",
        "colab_type": "text"
      },
      "source": [
        "We can see that it will output a 4 × 4 result as we expect, and importantly, the\n",
        "layer has no parameters or model weights. This is because it is not learning anything; it is just doubling the input. \n",
        "\n",
        "Finally, the model is used to upsample our input, resulting in a doubling of\n",
        "each row and column for our input data, as we expected.\n",
        "\n",
        "By default, the UpSampling2D will double each input dimension. This is defined by the size argument that is set to the tuple (2,2). You may want to use different factors on each dimension, such as double the width and triple the height. \n",
        "\n",
        "This could be achieved by setting the size argument to (2, 3). The result of applying this operation to a 2 × 2 image would be a 4 × 6 output image (e.g. 2 × 2 and 2 × 3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8rDHtIPXaD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f32e438c-e0bb-4f30-dda3-9d7e7bfc500e"
      },
      "source": [
        "# step-1: define input data\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [3, 4]          \n",
        "])\n",
        "print(X)\n",
        "\n",
        "# step-2: reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "\n",
        "# step-3: define model\n",
        "model = Sequential()\n",
        "model.add(UpSampling2D(input_shape=(2, 2, 1), size=(2, 3)))\n",
        "model.summary()\n",
        "\n",
        "# step-4: make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# step-5: reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 6))\n",
        "print(yhat)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "up_sampling2d_5 (UpSampling2 (None, 4, 6, 1)           0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[1. 1. 1. 2. 2. 2.]\n",
            " [1. 1. 1. 2. 2. 2.]\n",
            " [3. 3. 3. 4. 4. 4.]\n",
            " [3. 3. 3. 4. 4. 4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI3GI24wY5Fk",
        "colab_type": "text"
      },
      "source": [
        "Additionally, by default, the UpSampling2D layer will use a nearest neighbor algorithm to fill in the new rows and columns. This has the effect of simply doubling rows and columns, as described and is specified by the interpolation argument set to ‘nearest’. \n",
        "\n",
        "Alternately, a bilinear interpolation method can be used which draws upon multiple surrounding points. This can be specified via setting the interpolation argument to ‘bilinear’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K70ARkSEYXPb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "78255e9f-bbac-4dec-c2e7-4e8681af83c3"
      },
      "source": [
        "# step-1: define input data\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [3, 4]          \n",
        "])\n",
        "print(X)\n",
        "\n",
        "# step-2: reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "\n",
        "# step-3: define model\n",
        "model = Sequential()\n",
        "model.add(UpSampling2D(input_shape=(2, 2, 1), size=(2, 3), interpolation='bilinear'))\n",
        "model.summary()\n",
        "\n",
        "# step-4: make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# step-5: reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 6))\n",
        "print(yhat)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "up_sampling2d_6 (UpSampling2 (None, 4, 6, 1)           0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[1.        1.        1.3333334 1.6666667 2.        2.       ]\n",
            " [1.5       1.5       1.8333334 2.1666667 2.5       2.5      ]\n",
            " [2.5       2.5       2.8333335 3.1666667 3.5       3.5      ]\n",
            " [3.        3.        3.3333335 3.6666667 4.        4.       ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CprcH7ShZVWl",
        "colab_type": "text"
      },
      "source": [
        "### Simple Generator Model With the UpSampling2D Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdoxUpyPZWUF",
        "colab_type": "text"
      },
      "source": [
        "The UpSampling2D layer is simple and effective, although does not perform any learning. It is not able to fill in useful detail in the upsampling operation. To be useful in a GAN, each UpSampling2D layer must be followed by a Conv2D layer that will learn to interpret the doubled input and be trained to translate it into meaningful detail.\n",
        "\n",
        "In this example, our little GAN generator model must produce a 10 × 10 image as output and take a 100 element vector of random numbers from the latent space as input. \n",
        "\n",
        "First, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5 × 5 image.\n",
        "\n",
        "Next, the 5 × 5 feature maps can be upsampled to a 10 × 10 feature map.\n",
        "\n",
        "Finally, the upsampled feature maps can be interpreted and filled in with hopefully useful detail by a Conv2D layer. The Conv2D has a single feature map as output to create the single image we require.\n",
        "\n",
        "Tying this together, the complete example is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnO_d7afZEwZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "381d5695-e47b-403a-d4dd-c2088d10bc3b"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "\n",
        "# define input shape, output enough activations for 128 5x5 image\n",
        "model.add(Dense(128 * 5 * 5, input_dim=100))\n",
        "\n",
        "# reshape vector of activations into 128 feature maps with 5x5\n",
        "model.add(Reshape((5, 5, 128)))\n",
        "\n",
        "# double input from 128 5x5 to 1 10x10 feature map\n",
        "model.add(UpSampling2D())\n",
        "\n",
        "# fill in detail in the upsampled feature maps and output a single image\n",
        "model.add(Conv2D(1, (3, 3), padding='SAME'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3200)              323200    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_7 (UpSampling2 (None, 10, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 10, 10, 1)         1153      \n",
            "=================================================================\n",
            "Total params: 324,353\n",
            "Trainable params: 324,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8y0BrAhbDZN",
        "colab_type": "text"
      },
      "source": [
        "We can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5×5. The widths and heights are doubled to 10×10 by the UpSampling2D layer, resulting in a feature map with quadruple the area. \n",
        "\n",
        "Finally, the Conv2D processes these feature maps and adds in detail, outputting a single 10 × 10 image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sfG77xwbOJz",
        "colab_type": "text"
      },
      "source": [
        "## How to Use the Transpose Convolutional Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmpZ9MyqbPBm",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hef_eLV4a1Ue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}