{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-upsampling-and-transpose-convolutional-for-generative-adversarial-networks.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP706+k34Foa7UO0Fiv6JLz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/generative-adversarial-networks-with-python/blob/part-1-foundations/1_upsampling_and_transpose_convolutional_for_generative_adversarial_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO2OoKmSR0WT",
        "colab_type": "text"
      },
      "source": [
        "# Upsampling and Transpose Convolutional  for Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI5OT5XTR5sR",
        "colab_type": "text"
      },
      "source": [
        "The GAN architecture is comprised of both a generator and a discriminator model. The generator is responsible for creating new outputs, such as images, that plausibly could have come from the original dataset. The generator model is typically implemented using a deep convolutional neural network and\n",
        "results-specialized layers that learn to fill in features in an image rather than extract features from an input image.\n",
        "\n",
        "Two common types of layers that can be used in the generator model are a upsample layer that simply doubles the dimensions of the input and the transpose convolutional layer that performs an inverse convolution operation.\n",
        "\n",
        "In this notebook, we will discover how to use Upsampling and Transpose Convolutional Layers in Generative Adversarial Networks when generating images. \n",
        "\n",
        "After completing this guide, we will know:\n",
        "\n",
        "* Generative models in the GAN architecture are required to upsample input data in order to generate an output image.\n",
        "\n",
        "* The Upsampling layer is a simple layer with no weights that will double the dimensions of input and can be used in a generative model when followed by a traditional convolutional layer.\n",
        "\n",
        "* The Transpose Convolutional layer is an inverse convolutional layer that will both upsample input and learn how to fill in details during the model training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS9gL_XJTbQl",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-FndCvuTcXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import UpSampling2D, Dense, Reshape, Conv2D, Conv2DTranspose\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHZm8KRUTtMj",
        "colab_type": "text"
      },
      "source": [
        "## Need for Upsampling in GANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I0iJRRNTw79",
        "colab_type": "text"
      },
      "source": [
        "Generative Adversarial Networks are an architecture for neural networks for training a generative model. The architecture is comprised of a generator and a discriminator model, each of which are implemented as a deep convolutional neural network. \n",
        "\n",
        "The discriminator is responsible for classifying images as either real (from the domain) or fake (generated). \n",
        "\n",
        "The generator is responsible for generating new plausible examples from the problem domain. The generator works by taking a random point from the latent space as input and outputting a complete image, in a one-shot manner.\n",
        "\n",
        "A traditional convolutional neural network for image classification, and related tasks, will use pooling layers to downsample input images. For example, an average pooling or max pooling layer will reduce the feature maps from a convolutional by half on each dimension, resulting in an output that is one quarter the area of the input. \n",
        "\n",
        "Convolutional layers themselves also perform a form of downsampling by applying each filter across the input images or feature maps; the resulting activations are an output feature map that is smaller because of the border effects.\n",
        "Often padding is used to counter this effect. \n",
        "\n",
        "The generator model in a GAN requires an inverse operation of a pooling layer in a traditional convolutional layer. It needs a layer to translate\n",
        "from coarse salient features to a more dense and detailed output.\n",
        "\n",
        "A simple version of an unpooling or opposite pooling layer is called an upsampling layer. It works by repeating the rows and columns of the input. A more elaborate approach is to perform a backwards convolutional operation, originally referred to as a deconvolution, which is incorrect, but is more commonly referred to as a fractional convolutional layer or a transposed\n",
        "convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHrMP7j7Ug21",
        "colab_type": "text"
      },
      "source": [
        "## How to Use the Upsampling Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYHTtuCTUh6p",
        "colab_type": "text"
      },
      "source": [
        "Perhaps the simplest way to upsample an input is to double each row and column. \n",
        "\n",
        "For example, an input image with the shape 2 × 2 would be output as 4 × 4.\n",
        "\n",
        "```python\n",
        "input = [\n",
        "  [1, 2],\n",
        "  [3, 4]\n",
        "]\n",
        "\n",
        "Output = [\n",
        "  [1, 1, 2, 2],\n",
        "  [1, 1, 2, 2],\n",
        "  [3, 3, 4, 4],\n",
        "  [3, 3, 4, 4]\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb0OhdQiVTM_",
        "colab_type": "text"
      },
      "source": [
        "### Worked Example Using the UpSampling2D Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwIR4BHHVUOB",
        "colab_type": "text"
      },
      "source": [
        "The Keras deep learning library provides this capability in a layer called UpSampling2D. It can be added to a convolutional neural network and repeats the rows and columns provided as input in the output.\n",
        "\n",
        "**Step-1**\n",
        "\n",
        "First, we can define a contrived input image that is 2 × 2 pixels. We can use specific values for each pixel so that after upsampling, we can see exactly what effect the operation had on the input.\n",
        "\n",
        "**Step-2**\n",
        "\n",
        "Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model. The data dimensions in order are: samples, rows, columns, and channels.\n",
        "\n",
        "**Step-3**\n",
        "\n",
        "We can now define our model. The model has only the UpSampling2D layer which takes 2 × 2 grayscale images as input directly and outputs the result of the upsampling operation.\n",
        "\n",
        "**Step-4**\n",
        "\n",
        "We can then use the model to make a prediction, that is upsample a provided input image.\n",
        "\n",
        "**Step-5**\n",
        "\n",
        "The output will have four dimensions, like the input, therefore, we can convert it back to a 2 × 2 array to make it easier to review the result.\n",
        "\n",
        "Tying all of this together, the complete example of using the UpSampling2D layer in Keras is provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTLyuHPTWL4U",
        "colab_type": "code",
        "outputId": "4af0828a-049c-4d9c-e85f-20d352e7d347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# step-1: define input data\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [3, 4]          \n",
        "])\n",
        "print(X)\n",
        "\n",
        "# step-2: reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "\n",
        "# step-3: define model\n",
        "model = Sequential()\n",
        "model.add(UpSampling2D(input_shape=(2, 2, 1)))\n",
        "model.summary()\n",
        "\n",
        "# step-4: make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# step-5: reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 4))\n",
        "print(yhat)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "up_sampling2d (UpSampling2D) (None, 4, 4, 1)           0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[1. 1. 2. 2.]\n",
            " [1. 1. 2. 2.]\n",
            " [3. 3. 4. 4.]\n",
            " [3. 3. 4. 4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAWYZxw1Xn1_",
        "colab_type": "text"
      },
      "source": [
        "We can see that it will output a 4 × 4 result as we expect, and importantly, the\n",
        "layer has no parameters or model weights. This is because it is not learning anything; it is just doubling the input. \n",
        "\n",
        "Finally, the model is used to upsample our input, resulting in a doubling of\n",
        "each row and column for our input data, as we expected.\n",
        "\n",
        "By default, the UpSampling2D will double each input dimension. This is defined by the size argument that is set to the tuple (2,2). You may want to use different factors on each dimension, such as double the width and triple the height. \n",
        "\n",
        "This could be achieved by setting the size argument to (2, 3). The result of applying this operation to a 2 × 2 image would be a 4 × 6 output image (e.g. 2 × 2 and 2 × 3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8rDHtIPXaD9",
        "colab_type": "code",
        "outputId": "e99c99c9-9f0d-4e58-e9d8-90f4607e0df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# step-1: define input data\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [3, 4]          \n",
        "])\n",
        "print(X)\n",
        "\n",
        "# step-2: reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "\n",
        "# step-3: define model\n",
        "model = Sequential()\n",
        "model.add(UpSampling2D(input_shape=(2, 2, 1), size=(2, 3)))\n",
        "model.summary()\n",
        "\n",
        "# step-4: make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# step-5: reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 6))\n",
        "print(yhat)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "up_sampling2d_1 (UpSampling2 (None, 4, 6, 1)           0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[1. 1. 1. 2. 2. 2.]\n",
            " [1. 1. 1. 2. 2. 2.]\n",
            " [3. 3. 3. 4. 4. 4.]\n",
            " [3. 3. 3. 4. 4. 4.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI3GI24wY5Fk",
        "colab_type": "text"
      },
      "source": [
        "Additionally, by default, the UpSampling2D layer will use a nearest neighbor algorithm to fill in the new rows and columns. This has the effect of simply doubling rows and columns, as described and is specified by the interpolation argument set to ‘nearest’. \n",
        "\n",
        "Alternately, a bilinear interpolation method can be used which draws upon multiple surrounding points. This can be specified via setting the interpolation argument to ‘bilinear’."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K70ARkSEYXPb",
        "colab_type": "code",
        "outputId": "f87c807d-0ff5-41a3-92c0-63a1643bd96b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# step-1: define input data\n",
        "X = np.array([\n",
        "    [1, 2],\n",
        "    [3, 4]          \n",
        "])\n",
        "print(X)\n",
        "\n",
        "# step-2: reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "\n",
        "# step-3: define model\n",
        "model = Sequential()\n",
        "model.add(UpSampling2D(input_shape=(2, 2, 1), size=(2, 3), interpolation='bilinear'))\n",
        "model.summary()\n",
        "\n",
        "# step-4: make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# step-5: reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 6))\n",
        "print(yhat)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "up_sampling2d_2 (UpSampling2 (None, 4, 6, 1)           0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[1.        1.        1.3333334 1.6666667 2.        2.       ]\n",
            " [1.5       1.5       1.8333334 2.1666667 2.5       2.5      ]\n",
            " [2.5       2.5       2.8333335 3.1666667 3.5       3.5      ]\n",
            " [3.        3.        3.3333335 3.6666667 4.        4.       ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CprcH7ShZVWl",
        "colab_type": "text"
      },
      "source": [
        "### Simple Generator Model With the UpSampling2D Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdoxUpyPZWUF",
        "colab_type": "text"
      },
      "source": [
        "The UpSampling2D layer is simple and effective, although does not perform any learning. It is not able to fill in useful detail in the upsampling operation. To be useful in a GAN, each UpSampling2D layer must be followed by a Conv2D layer that will learn to interpret the doubled input and be trained to translate it into meaningful detail.\n",
        "\n",
        "In this example, our little GAN generator model must produce a 10 × 10 image as output and take a 100 element vector of random numbers from the latent space as input. \n",
        "\n",
        "First, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5 × 5 image.\n",
        "\n",
        "Next, the 5 × 5 feature maps can be upsampled to a 10 × 10 feature map.\n",
        "\n",
        "Finally, the upsampled feature maps can be interpreted and filled in with hopefully useful detail by a Conv2D layer. The Conv2D has a single feature map as output to create the single image we require.\n",
        "\n",
        "Tying this together, the complete example is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnO_d7afZEwZ",
        "colab_type": "code",
        "outputId": "f790fa64-3e1c-4804-afcb-16e6f6399bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "\n",
        "# define input shape, output enough activations for 128 5x5 image\n",
        "model.add(Dense(128 * 5 * 5, input_dim=100))\n",
        "\n",
        "# reshape vector of activations into 128 feature maps with 5x5\n",
        "model.add(Reshape((5, 5, 128)))\n",
        "\n",
        "# double input from 128 5x5 to 1 10x10 feature map\n",
        "model.add(UpSampling2D())\n",
        "\n",
        "# fill in detail in the upsampled feature maps and output a single image\n",
        "model.add(Conv2D(1, (3, 3), padding='SAME'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 3200)              323200    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 10, 10, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 10, 10, 1)         1153      \n",
            "=================================================================\n",
            "Total params: 324,353\n",
            "Trainable params: 324,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8y0BrAhbDZN",
        "colab_type": "text"
      },
      "source": [
        "We can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5×5. The widths and heights are doubled to 10×10 by the UpSampling2D layer, resulting in a feature map with quadruple the area. \n",
        "\n",
        "Finally, the Conv2D processes these feature maps and adds in detail, outputting a single 10 × 10 image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sfG77xwbOJz",
        "colab_type": "text"
      },
      "source": [
        "## How to Use the Transpose Convolutional Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmpZ9MyqbPBm",
        "colab_type": "text"
      },
      "source": [
        "The transpose convolutional layer is more complex than a simple upsampling layer. A simple way to think about it is that it both performs the upsample operation and interprets the coarse input data to fill in the detail while it is upsampling. It is like a layer that combines the UpSampling2D and Conv2D layers into one layer. This is a crude understanding, but a practical starting point.\n",
        "\n",
        "In fact, the transpose convolutional layer performs an inverse convolution operation. Specifically, the forward and backward passes of the convolutional layer are reversed.\n",
        "\n",
        "Let’s make this concrete with an example. Consider an input image with the size 2 × 2 as follows:\n",
        "\n",
        "```python\n",
        "\n",
        "        1, 2\n",
        "Input = 3, 4\n",
        "```\n",
        "\n",
        "Assuming a single filter with a 1 × 1 kernel and model weights that result in no changes to the inputs when output\n",
        "\n",
        "```python\n",
        "\n",
        "         1, 2\n",
        "Output = 3, 4\n",
        "```\n",
        "\n",
        "With an output stride of (2,2), the 1 × 1 convolution requires the insertion of additional rows and columns into the input image so that the reads of the operation can be performed.\n",
        "\n",
        "```python\n",
        "\n",
        "        1, 0, 2, 0\n",
        "Input = 0, 0, 0, 0\n",
        "        3, 0, 4, 0\n",
        "        0, 0, 0, 0\n",
        "```\n",
        "\n",
        "The model can then read across this input using an output stride of (2,2) and will output a 4 × 4 image, in this case with no change as our model weights have no effect by design:\n",
        "\n",
        "```python\n",
        "\n",
        "        1, 0, 2, 0\n",
        "Output= 0, 0, 0, 0\n",
        "        3, 0, 4, 0\n",
        "        0, 0, 0, 0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSg77CiXkhIf",
        "colab_type": "text"
      },
      "source": [
        "### Worked Example Using the Conv2DTranspose Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4lRm1tk0Y0",
        "colab_type": "text"
      },
      "source": [
        "Keras provides the transpose convolution capability via the Conv2DTranspose layer.\n",
        "\n",
        "**Step-1**:First, we can define a contrived input image that is 2 × 2 pixels.\n",
        "\n",
        "**Step-2**:Once the image is defined, we must add a channel dimension (e.g. grayscale) and also a sample dimension (e.g. we have 1 sample) so that we can pass it as input to the model.\n",
        "\n",
        "**Step-3**:We can now define our model. The model has only the Conv2DTranspose layer, which takes 2 × 2 grayscale images as input directly and outputs the result of the operation. The Conv2DTranspose both upsamples and performs a convolution. Additionally, we must specify a stride of (2,2) because the upsampling is achieved by the stride behavior of the convolution on the input. Specifying a stride of (2,2) has the effect of spacing out the input.\n",
        "\n",
        "**Step-4**:To make it clear what the Conv2DTranspose layer is doing, we will fix the single weight in the single filter to the value of 1.0 and use a bias value of 0.0. These weights, along with a kernel size of (1,1) will mean that values in the input will be multiplied by 1 and output as-is, and the 0 values in the new rows and columns added via the stride of 2 × 2 will be output as 0.\n",
        "\n",
        "Tying all of this together, the complete example of using the Conv2DTranspose layer in Keras is provided below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hef_eLV4a1Ue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fffb2736-cf3a-4fb2-b6aa-7f1daf29a108"
      },
      "source": [
        "# Step-1 : define input data\n",
        "X = np.array([[1, 2], [3, 4]])\n",
        "print(X)\n",
        "\n",
        "# Step-2: reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "\n",
        "# Step-3: define model\n",
        "model = Sequential()\n",
        "model.add(Conv2DTranspose(1, (1, 1), strides=(2, 2), input_shape=(2, 2, 1)))\n",
        "model.summary()\n",
        "\n",
        "# Step-4 : define weights that they do nothing\n",
        "weights = [np.array([[[[1]]]]), np.array([0])]\n",
        "# store the weights in the model\n",
        "model.set_weights(weights)\n",
        "\n",
        "# make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 4))\n",
        "print(yhat)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_transpose (Conv2DTran (None, 4, 4, 1)           2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[1. 0. 2. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [3. 0. 4. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaKaF0h2jYxt",
        "colab_type": "text"
      },
      "source": [
        "We can see that it will output a 4 × 4 result as we expect, and importantly, the\n",
        "layer two parameters or model weights. One for the single 1×1 filter and one for the bias. Unlike the UpSampling2D layer, the Conv2DTranspose will learn during training and will attempt to fill in detail as part of the upsampling process. Finally, the model is used to upsample our input.\n",
        "\n",
        "In practice, we will use a large number of filters (e.g. 64 or 128), a larger kernel (e.g. 3 × 3, 5 × 5, etc.), and the layer will be initialized with random weights that will learn how to effectively upsample with detail during training. \n",
        "\n",
        "In fact, you might imagine how different sized kernels will result in different\n",
        "sized outputs, more than doubling the width and height of the input. In this case, the padding argument of the layer can be set to ‘same’ to force the output to have the desired (doubled) output shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlLSru-2o8Y9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "fe0533d8-bcac-40b4-8e53-6fe64b1fb053"
      },
      "source": [
        "# Step-1 : define input data\n",
        "X = np.array([[1, 2], [3, 4]])\n",
        "print(X)\n",
        "\n",
        "# Step-2: reshape input data into one sample a sample with a channel\n",
        "X = X.reshape((1, 2, 2, 1))\n",
        "\n",
        "# Step-3: define model\n",
        "model = Sequential()\n",
        "# using padding to ensure that the output are only doubled\n",
        "model.add(Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='SAME', input_shape=(2, 2, 1)))\n",
        "model.summary()\n",
        "\n",
        "# Step-4 : define weights that they do nothing\n",
        "#weights = [np.array([[[[1]]]]), np.array([0])]\n",
        "#print(weights.shape)\n",
        "# store the weights in the model\n",
        "#model.set_weights(weights)\n",
        "\n",
        "# make a prediction with the model\n",
        "yhat = model.predict(X)\n",
        "\n",
        "# reshape output to remove channel to make printing easier\n",
        "yhat = yhat.reshape((4, 4))\n",
        "print(yhat)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 2]\n",
            " [3 4]]\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_transpose_6 (Conv2DTr (None, 4, 4, 1)           10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[-7.4414134e-02  2.8711915e-02 -1.5258479e-01  5.7423830e-02]\n",
            " [-4.9986732e-01 -1.6379261e-01 -8.9213449e-01 -3.2758522e-01]\n",
            " [ 1.6900164e-01 -1.1338845e-01 -6.0266256e-04 -2.8420073e-01]\n",
            " [-1.4996020e+00 -4.9137783e-01 -1.6766689e+00 -6.5517044e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHXU_KBxqT-v",
        "colab_type": "text"
      },
      "source": [
        "### Simple Generator Model With the Conv2DTranspose Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QCl1Mv_qWqi",
        "colab_type": "text"
      },
      "source": [
        "The Conv2DTranspose is more complex than the UpSampling2D layer, but it is also effective when used in GAN models, specifically the generator model. Either approach can be used, although the Conv2DTranspose layer is preferred, perhaps because of the simpler generator models and possibly better results, although GAN performance and skill is notoriously difficult to quantify.\n",
        "\n",
        "In this case, our little GAN generator model must produce a 10 × 10 image and take a 100-element vector from the latent space as input, as in the previous UpSampling2D example.\n",
        "\n",
        "First, a Dense fully connected layer can be used to interpret the input vector and create a sufficient number of activations (outputs) that can be reshaped into a low-resolution version of our output image, in this case, 128 versions of a 5 × 5 image.\n",
        "\n",
        "Next, the 5 × 5 feature maps can be upsampled to a 10 × 10 feature map. We will use a 3 × 3 kernel size for the single filter, which will result in a slightly larger than doubled width and height in the output feature map (11 × 11). Therefore, we will set the padding argument to ‘same’ to ensure the output dimensions are 10 × 10 as required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BBxeVdYpHuI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "60a905d1-7ee8-4570-cde6-a774c14ad747"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# define input shape, output enough activations for for 128 5x5 image\n",
        "model.add(Dense(128 * 5 * 5, input_dim=100))\n",
        "\n",
        "# reshape vector of activations into 128 feature maps with 5x5\n",
        "model.add(Reshape((5, 5, 128)))\n",
        "\n",
        "# double input from 128 5x5 to 1 10x10 feature map\n",
        "model.add(Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='SAME'))\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 3200)              323200    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 5, 5, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_7 (Conv2DTr (None, 10, 10, 1)         1153      \n",
            "=================================================================\n",
            "Total params: 324,353\n",
            "Trainable params: 324,353\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCWKv44yr5Cd",
        "colab_type": "text"
      },
      "source": [
        "We can see that the Dense layer outputs 3,200 activations that are then reshaped into 128 feature maps with the shape 5 × 5. The widths and heights are doubled to 10 × 10 by the Conv2DTranspose layer resulting in a single feature map with quadruple the area."
      ]
    }
  ]
}